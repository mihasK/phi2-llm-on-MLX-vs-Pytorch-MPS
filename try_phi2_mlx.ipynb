{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS = 200  # restriction for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check params used:\n",
      "MAX_TOKENS = 200\n"
     ]
    }
   ],
   "source": [
    "print('Check params used:')\n",
    "print('MAX_TOKENS =', MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arm\n"
     ]
    }
   ],
   "source": [
    "# Check output - should be ARM!\n",
    "# If it is i386 (and you have M series machine) then you are using a non-native Python. Switch your Python to a native Python. A good way to do this is with Conda.\n",
    "\n",
    "\n",
    "!python -c \"import platform; print(platform.processor())\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check current memory usage\n",
      "Cache emptied: python (GC) and MPS \n",
      "######## Memory consumption ########:\n",
      "        MPS tensors    MPS Total    Process Memory    Total System Memory\n",
      "------  -------------  -----------  ----------------  ---------------------\n",
      "Before  0              0            245               14292\n",
      "After   0              0            245               14292\n",
      "Diff    +0 MB          +0 MB        +0 MB             +0 MB\n"
     ]
    }
   ],
   "source": [
    "from utils import MPS_MemoryTracker\n",
    "\n",
    "print(\"Check current memory usage\")\n",
    "\n",
    "with MPS_MemoryTracker():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download weights and convert them into MLX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weights and convertin to MLX format:\n",
      "Cache emptied: python (GC) and MPS \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c925c8119eb24bd59898974889ab365d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache emptied: python (GC) and MPS \n",
      "######## Memory consumption ########:\n",
      "        MPS tensors    MPS Total    Process Memory    Total System Memory\n",
      "------  -------------  -----------  ----------------  ---------------------\n",
      "Before  0              0            267               14324\n",
      "After   0              0            2947              17789\n",
      "Diff    +0 MB          +0 MB        +2680 MB          +3465 MB\n",
      "peak memory: 9383.31 MiB, increment: 9138.03 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    " \n",
    "from phi2_mlx import convert\n",
    "# download weights and convert them into MLX format\n",
    "\n",
    "print(\"Downloading weights and convertin to MLX format:\")\n",
    "\n",
    "with MPS_MemoryTracker(clean_cache_before=True, clean_cache_after=True):\n",
    "    convert()\n",
    "\n",
    "# Unfortunately, this function leaks memory, which could be seen in the ouput of memory tracker \n",
    "\n",
    "# objgraph.show_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi2_mlx import load_model, generate, get_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of MLX model:\n",
      "Cache emptied: python (GC) and MPS \n",
      "######## Memory consumption ########:\n",
      "        MPS tensors    MPS Total    Process Memory    Total System Memory\n",
      "------  -------------  -----------  ----------------  ---------------------\n",
      "Before  0              0            2947              17815\n",
      "After   0              5560         6822              19793\n",
      "Diff    +0 MB          +5560 MB     +3875 MB          +1978 MB\n",
      "peak memory: 7301.50 MiB, increment: 4353.78 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "if 'model' in locals():\n",
    "    print('Delete existing model obj')\n",
    "    del model\n",
    "\n",
    "print(\"Loading of MLX model:\")\n",
    "with MPS_MemoryTracker():\n",
    "    model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[2061,  389,  262, 6994, 5087,  286,  838,   30]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "input_txt = 'What are the prime factors of 10?'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "inputs = tokenizer(input_txt, return_tensors=\"np\", return_attention_mask=False)\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "def generate_tokens(inputs, temp, max_tokens,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                    ):\n",
    "    inputs = mx.array(inputs[\"input_ids\"])\n",
    "\n",
    "    tokens = []\n",
    "    for token, ind in zip(generate(inputs, model, temp), range(max_tokens)):\n",
    "        if token.item() == eos_token_id:\n",
    "            print(f'---DEBUG--- EOS generated at {ind} position')\n",
    "            break\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inference:\n",
      "Cache emptied: python (GC) and MPS \n",
      "Cache emptied: python (GC) and MPS \n",
      "######## Memory consumption ########:\n",
      "        MPS tensors    MPS Total    Process Memory    Total System Memory\n",
      "------  -------------  -----------  ----------------  ---------------------\n",
      "Before  0              5560         6846              19833\n",
      "After   0              10052        4740              23719\n",
      "Diff    +0 MB          +4492 MB     -2106 MB          +3886 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%memit  # to use memit here, set TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "\n",
    "# Run `make track` during cell execution in mesure CPU/GPU usage\n",
    "print('Model inference:')\n",
    "with MPS_MemoryTracker(clean_cache_before=True, clean_cache_after=True):\n",
    "    tokens = generate_tokens(inputs, temp=0.2, max_tokens=MAX_TOKENS, eos_token_id=-1)\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time measurement for model inference:\n"
     ]
    }
   ],
   "source": [
    "print('Time measurement for model inference:')\n",
    "\n",
    "%timeit generate_tokens(inputs, temp=0.2, max_tokens=MAX_TOKENS, eos_token_id=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full text-to-text example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating with Phi-2 on MLX...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## INPUT\n",
      "Write a short poem about deep learning\n",
      "## Output\n",
      "---DEBUG--- EOS generated at 58 position\n",
      "Deep learning is a powerful tool\n",
      "That can learn from data and rules\n",
      "It can recognize patterns and features\n",
      "And make predictions with accuracy\n",
      "Deep learning is a fascinating field\n",
      "That can solve many problems and challenges\n",
      "It can inspire creativity and curiosity\n",
      "And make the world a better place\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_txt(input_txt: str, tokenizer, temp=0.2, max_tokens=MAX_TOKENS):\n",
    "    inputs = tokenizer(\n",
    "        input_txt,\n",
    "        return_tensors=\"np\",\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"[INFO] Generating with Phi-2 on MLX...\", flush=True)\n",
    "    print(input_txt, end=\"\", flush=True)\n",
    "\n",
    "    tokens = generate_tokens(inputs, temp, max_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    mx.eval(tokens)\n",
    "    s = tokenizer.decode([t.item() for t in tokens])\n",
    "    return s    \n",
    "\n",
    "input_txt = \"\"\"\n",
    "## INPUT\n",
    "Write a short poem about deep learning\n",
    "## Output\n",
    "\"\"\"\n",
    "text = generate_txt(input_txt, tokenizer, max_tokens=1000,)\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
